{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Image Caption Generator Model Using CNN an Transformer ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravalika1010/task1/blob/main/Image_Caption_Generator_Model_Using_CNN_an_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wefbfEp4I3_U"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T69FrORMHlLD"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "seed = 111\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5XJIJo1Jmxm"
      },
      "source": [
        "**Downloading the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM_MlSxYJzTT"
      },
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okmKh2qmKN5s"
      },
      "source": [
        "# Path to the images\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 20\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Number of self-attention heads\n",
        "NUM_HEADS = 2\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "LEARNING_RATE = 0.00001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPS4fHM0KWrs"
      },
      "source": [
        "**Preparing the dataset**\n",
        "\n",
        "load_captions_data():\n",
        "\n",
        "Loads captions (text) data and maps them to corresponding images.\n",
        "\n",
        "Args:filename: Path to the text file containing caption data.\n",
        "\n",
        "Returns:caption_mapping: Dictionary mapping image names and the corresponding captions.\n",
        "\n",
        "text_data: List containing all the available captions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWphVnY5Kea8"
      },
      "source": [
        "def load_captions_data(filename):     #\n",
        "  with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "            # Each image is repeated five times for the five different captions. Each\n",
        "            # image name has a prefix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            if img_name.endswith(\"jpg\"):\n",
        "                # We will add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        return caption_mapping, text_data\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z12A5EjTLN35"
      },
      "source": [
        "train_val_split():\n",
        "\n",
        "Split the captioning dataset into train and validation sets.\n",
        "\n",
        "Args:caption_data (dict): Dictionary containing the mapped caption data\n",
        "\n",
        "train_size (float): Fraction of all the full dataset to use as training data\n",
        "\n",
        "shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "Returns:Traning and validation datasets as two separated dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgqOhhZzLyyh",
        "outputId": "0d3ca00d-da2c-4e9e-f721-b9e0e18a1671"
      },
      "source": [
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  6472\n",
            "Number of validation samples:  1619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq365FKQMBhQ"
      },
      "source": [
        "**Vectorizing the text data**\n",
        "\n",
        "Text Vectorization is the process of converting text into numerical representation.\n",
        "\n",
        "To vectorize the text data, Â TextVectorization layer is utilisied, which will convert the original strings into integer sequences, with each integer representing the index of a word in a vocabulary. A custom string standardization scheme (strip punctuation characters except < and >) is used and the default scheme ( split on whitespace ) is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDvaAjE-ND1_"
      },
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,)\n",
        "vectorization.adapt(text_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmIpSJrINWDD"
      },
      "source": [
        "**Building a tf.data.Dataset pipeline for training**\n",
        "\n",
        "Pairs of images are generated and corresponding captions using a tf.data.Dataset object. \n",
        "\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Read the image from the disk\n",
        "2.   Tokenize all the five captions corresponding to the image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0uAaUl-NTzU"
      },
      "source": [
        "def read_image(img_path, size=IMAGE_SIZE):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlAsNFXQQrXT"
      },
      "source": [
        "def make_dataset(images, captions):\n",
        "    img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
        "        read_image, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
        "        vectorization, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
        "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XuoWv6cRG0o"
      },
      "source": [
        "**Building the model**\n",
        "\n",
        "The image captioning architecture consists of three models:\n",
        "\n",
        "\n",
        "\n",
        "1. **CNN:** used to extract the image features\n",
        "2. **TransformerEncoder:** The extracted image features are then passed to a Transformer based encoder that generates a new representation of the inputs\n",
        "3. **TransformerDecoder:** This model takes the encoder output and the text data (sequences) as inputs and tries to learn to generate the caption.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEYOSyypRINX"
      },
      "source": [
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
        "    )\n",
        "    # We freeze our feature extractor\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, 1280))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUDh6sSoSK2x"
      },
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = layers.Dense(embed_dim, activation=\"relu\")\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.dense_proj(inputs)\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=None\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        return proj_input"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLBZiRoOSO-N"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodA8NgQSVAv"
      },
      "source": [
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE)\n",
        "        self.dropout_1 = layers.Dropout(0.1)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        inputs = self.dropout_1(inputs, training=training)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=combined_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        proj_out = self.layernorm_3(out_2 + proj_output)\n",
        "        proj_out = self.dropout_2(proj_out, training=training)\n",
        "\n",
        "        preds = self.out(proj_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxo1BZWISeOe"
      },
      "source": [
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, cnn_model, encoder, decoder, num_captions_per_image=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # 3. Pass image embeddings to encoder\n",
        "                encoder_out = self.encoder(img_embed, training=True)\n",
        "\n",
        "                batch_seq_inp = batch_seq[:, i, :-1]\n",
        "                batch_seq_true = batch_seq[:, i, 1:]\n",
        "\n",
        "                # 4. Compute the mask for the input sequence\n",
        "                mask = tf.math.not_equal(batch_seq_inp, 0)\n",
        "\n",
        "                # 5. Pass the encoder outputs, sequence inputs along with\n",
        "                # mask to the decoder\n",
        "                batch_seq_pred = self.decoder(\n",
        "                    batch_seq_inp, encoder_out, training=True, mask=mask\n",
        "                )\n",
        "\n",
        "                # 6. Calculate loss and accuracy\n",
        "                caption_loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "                caption_acc = self.calculate_accuracy(\n",
        "                    batch_seq_true, batch_seq_pred, mask\n",
        "                )\n",
        "\n",
        "                # 7. Update the batch loss and batch accuracy\n",
        "                batch_loss += caption_loss\n",
        "                batch_acc += caption_acc\n",
        "\n",
        "            # 8. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 9. Get the gradients\n",
        "            grads = tape.gradient(caption_loss, train_vars)\n",
        "\n",
        "            # 10. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        loss = batch_loss\n",
        "        acc = batch_acc / float(self.num_captions_per_image)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            # 3. Pass image embeddings to encoder\n",
        "            encoder_out = self.encoder(img_embed, training=False)\n",
        "\n",
        "            batch_seq_inp = batch_seq[:, i, :-1]\n",
        "            batch_seq_true = batch_seq[:, i, 1:]\n",
        "\n",
        "            # 4. Compute the mask for the input sequence\n",
        "            mask = tf.math.not_equal(batch_seq_inp, 0)\n",
        "\n",
        "            # 5. Pass the encoder outputs, sequence inputs along with\n",
        "            # mask to the decoder\n",
        "            batch_seq_pred = self.decoder(\n",
        "                batch_seq_inp, encoder_out, training=False, mask=mask\n",
        "            )\n",
        "\n",
        "            # 6. Calculate loss and accuracy\n",
        "            caption_loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "            caption_acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "            # 7. Update the batch loss and batch accuracy\n",
        "            batch_loss += caption_loss\n",
        "            batch_acc += caption_acc\n",
        "\n",
        "        loss = batch_loss\n",
        "        acc = batch_acc / float(self.num_captions_per_image)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ4N29a6Sg7j",
        "outputId": "d7d2c5c8-15a3-4e7d-d2d1-59ebfce9fab1"
      },
      "source": [
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(\n",
        "    embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "decoder = TransformerDecoderBlock(\n",
        "    embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY8Wh4UnSoCE"
      },
      "source": [
        "**Model training**\n",
        "\n",
        "Here ,the loss function is defined using cross entropy.\n",
        "\n",
        "Callbacks provide a way to execute code and interact with the training model process automatically.\n",
        "\n",
        "early_stopping: Keras supports the early stopping of training via a callback called EarlyStopping.\n",
        "\n",
        "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
        "\n",
        "The EarlyStopping callback is configured when instantiated via arguments.\n",
        "\n",
        "Next model fitting is done.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpJWrOD9TD5S",
        "outputId": "70029466-7371-4219-fca3-8a7f718689b5"
      },
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=cross_entropy\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "102/102 [==============================] - 216s 1s/step - loss: 34.6713 - acc: 0.1030 - val_loss: 27.8442 - val_acc: 0.2239\n",
            "Epoch 2/30\n",
            "102/102 [==============================] - 172s 1s/step - loss: 25.8428 - acc: 0.2310 - val_loss: 23.9932 - val_acc: 0.2734\n",
            "Epoch 3/30\n",
            "102/102 [==============================] - 165s 1s/step - loss: 23.2083 - acc: 0.2833 - val_loss: 22.0704 - val_acc: 0.3084\n",
            "Epoch 4/30\n",
            "102/102 [==============================] - 164s 1s/step - loss: 21.7424 - acc: 0.3120 - val_loss: 20.9651 - val_acc: 0.3268\n",
            "Epoch 5/30\n",
            " 57/102 [===============>..............] - ETA: 53s - loss: 20.9709 - acc: 0.3289"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mufq2Nj7UYEM"
      },
      "source": [
        "**Checking sample predictions**\n",
        "\n",
        "Caption is generated using th transformer decoder and predictions are done on various random images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ccBH1hThf2y"
      },
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = read_image(sample_img)\n",
        "    img = sample_img.numpy().astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \" <end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    print(\"PREDICTED CAPTION:\", end=\" \")\n",
        "    print(decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip())\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}